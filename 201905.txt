Resilience4J-Retry组件：
	maxAttempts: 默认值3，最大重试次数
	waitDuration: 默认值500ms, 固定重试间隔
	intervalFunction: 默认值numberOfAttempts->waitDuration，用来改变重试时间间隔，可以选择指数退避或者随机时间间隔
	retryOnResultPredicate: result->false, 自定义结果重试规则，需要重试的返回true
	retryOnExceptionPredicate: throwable -> true, 自定义异常重试规则，需要重试的返回true
	retryExceptions: empty，需要重试的异常列表
	ignoreExceptions: empty, 需要忽略的异常列表

如果Resilience4J的Retry, CircuitBreaker, BulkHead, RateLimiter同时注解在方法上 ，默认的顺序是Retry>CircuitBreaker>RateLimiter>Bulkhead，即先控制并发再限流然后熔断最后重试。

实现通道相关系统间联动：支付通道故障时，一方面通过消息组件通知到营销活动、退款等系统，协助进行活动下线、通道退款关闭等处理，减少通道故障对其他系统的影响；另一方面以接口方式通知业务方系统，协助业务方进行故障分析。

Unsafe是位于sun.misc包下的一个类，主要提供一些用于执行低级别、不安全操作的方法，如直接访问系统内存资源、自主管理内存资源等，这些方法在提升Java运行效率、增强Java语言底层资源操作能力方面起到了很大的作用。但由于Unsafe类使用Java语言拥有了类似C语言指针一样操作内存空间的能力，无疑增加了程序发生相关指针问题的风险。在程序中过度、不正确使用Unsafe类会使得程序出错的概率变大。
Unsafe提供的API可分为内存操作、CAS、Class相关、对象操作、线程调度、系统信息获取、内存屏障、数组操作等几类。

通常，我们在Java中创建的对象都处于堆内内存（heap）中，堆内内存是由JVM所管控的Java进程内存，并且它们遵循JVM的内存管理机制，JVM会采用垃圾回收机制统一管理堆内存。与之相对的是堆外内存，存在于JVM管控之外的内存区域，Java中对堆外内存的操作，依赖于Unsafe提供的操作堆外内存的native方法。

使用堆外内存的原因：
	1. 对垃圾回收停顿的改善。由于堆外内存是直接受操作系统管理而不是JVM，所以当使用堆外内存时，即可保持较小的堆内内存规模，从而在GC时减少回收停顿对于应用的影响；
	2. 提升程序IO操作的性能。在通常IO通信过程中，会存在堆内内存到堆外内存的数据拷贝操作，对于需要频繁进行内存空间数据拷贝且生命周期较短的暂存数据，都建议存储到堆外内存。

DirectByteBuffer是Java用于实现堆外内存的一个重要类，通常用在通信过程中做缓冲池，如在Netty、MINA等NIO框架中应用广泛。DirectByteBuffer对于堆外内存的创建、使用、销毁等逻辑均由Unsafe提供的堆外内存API来实现。

CAS是一条CPU的原子指令(cmpxchg指令)，不会造成数据不一致问题，Unsafe提供的CAS方法(compareAndSwap***)底层实现即为CPU指令cmpxchg。

在Lambda表达式实现上，通过invokedynamic指令调用引导方法生成调用点，在此过程中，会通过ASM动态生成字节码，而后利用Unsafe的defineAnonymousClass方法实现相应的函数式接口的匿名类，然后再实例化此匿名类，并返回此匿名类中函数式方法的方法句柄关联的调用点；而后可以通过此调用点实现相应Lambda表达式定义逻辑的功能。

Unsafe类的public native Object allocateInstance() throws InstantiationException;绕过构造方法、初始化代码来创建对象。

Unsafe中提供allocateInstance方法，仅通过Class对象就可以创建此类的实例对象，而且不需要调用其构造函数、初始化代码、JVM安全检查等。它抑制修饰符检测，也就是即使构造器是private修饰的也能通过此方法实例化，只需提供类对象即可创建相应的对象。由于这种特性，allocateInstane在java.lang.invoke, Objenesis(提供绕过构造器的对象生成方式), Gson(反序列化时用到)中都有相应的应用。

Unsafe类与数组相关的arrayBaseOffset与arrayIndexScale两个方法，两者配合起来使用，即可定位数组中每个元素在内存中的位置。这两个和数组操作相关的方法，在AtomicIntegerArray(可以实现对Integer数组中每个元素的原子性操作)中有典型的应用。通过Unsafe的arrayBaseOffset, arrayIndexScale分别获取数组首元素的偏移地址base及单个元素大小因子scale。后续相关原子性操作，均依赖于这两个值进行数组中元素的定位。如getAndAdd方法即通过checkedByteOffset方法获取某数组元素的偏移地址，而后通过CAS实现原子性操作。

在JDK8中Unsafe引入内存屏障，用于定义内存屏障(也称为内存栅栏，内存栅障，屏障指令等，是一类同步屏障指令，是CPU或者编译器在对内存随机访问的操作中的一个同步点，使得此点之前的所有读写操作都执行后才可以开始执行此点之后的操作)，避免代码重排序。

//内存屏障，禁止load操作重排序。屏障前的load操作不能被重排序到屏障后，屏障后的load操作不能重排序到屏障前
public native void loadFence();
//内存屏障，禁止store操作重排序。屏障前的store操作不能重排序到屏障后，屏障后的store操作不能重排序到屏障前
public native void storeFence();
//内存屏障，禁止load,store操作重排序
public native void fullFence();

在JDK8中引入了一种锁的新机制：StampedLock，可以看成是读写锁的一个改进版本。StampedLock提供了一种乐观读锁的实现，这种乐观读锁类似于无锁的操作，完全不会阻塞写线程获取写锁，从而缓解读多写少时写线程"饥饿"现象。由于StampedLock提供的乐观读锁不阻塞写线程获取读锁，当线程共享变量从主内存load到工作内存时，会存在数据不一致问题，所以当使用StampedLock的乐观锁时，需要遵从使用模式来确保数据的一致性。

JVM规范要求每一个字节码文件都要由十部分按照固定顺序组成，整体结构如下：
	1. 魔数: Magic
	2. 版本号: Version
	3. 常量池：constant_pool
	4. 访问标志：access_flag
	5. 当前类索引：this_class
	6. 父类索引：super_class
	7. 接口索引：interface
	8. 字段表：fields
	9. 方法表：methods
	10. 附加属性：attributes

紧跟着主版本号之后的常量池入口。常量池中存储两类常量：字面量与符号引用。字面量为代码中声明为final的常量值，符号引用如类和接口的全局限定名、字段的名称和描述符、方法的名称和描述符。常量池整体上分为两部分：常量池计数器以及常量池数据区。

操作数栈和字节码：
JVM的指令集是基于栈而不是寄存器，基于栈可以具备很好的跨平台性(因为寄存器指令集往往和硬件挂钩)，但是缺点在于，要完成同样的操作，基于栈的实现需要更多指令才能完成(因为栈只是一个LIFO结构，需要频繁压栈出栈)。另外，由于栈是在内存实现的，而寄存器是在CPU的高速缓存区，相比较而言，基于栈的速度要慢很多，这也是为了跨平台而做出的牺牲。

字节码增强技术：
	1. AOP
	2. ASM
	3. CGLIB
	4. AspectJ
	5. Java Proxy 
	6. javassist
	7. Instrumentation 

ASM是在指令层次上操作字节码，在指令层次上操作字节码的框架实现起来非常晦涩。另一个源代码层次操作字节码的框架：Javassist。

Instrument是JVM提供的一个可以修改已加载类的类库，专门为Java语言编写的插桩服务提供支持。它需要依赖JVMTI的Attach API机制实现。在JDK1.6之前，Instrument只能在JVM刚启动开始加载类时生效，而在JDK1.6之后，Instrument支持了在运行时对类定义的修改。要使用Instrument的类修改功能，需要实现它提供的ClassFileTransformer接口，定义一个类文件转换器。接口中的transform()方法会在类文件被加载时调用，而在transform方法里利用ASM或Javassist对传入的字节码进行改写或替换，生成新的字节码数组后返回。

使用Java层面的工具定位内存区域(堆内内存，Code区域或者使用unsafe.allocateMemory和DirectByteBuffer申请的堆外内存)
在项目中添加-XX:NativeMemoryTracking=detail JVM参数重启JVM，使用命令jcmd <pid> VM.native_memory_detail查看内存分布。

pmap查看内存分布 

降低软件复杂性的一般原则和方法 

HashMap线程不安全的场景：
	1. 多线程put操作后，get操作导致死循环(JDK7中)；
	2. 多线程put非空元素后，get操作得到NULL值；
	3. 多线程put操作，导致元素丢失。

临键锁：Next-Key Lock

InnoDB下行锁可细分为：记录锁Record Lock, 间隙锁Gap Lock, 临键锁Next-Key Lock。是基于索引实现的。
临键锁：记录锁与间隙锁组合起来用就叫做Next-Key Lock，就是将键及其两边的间隙加锁(向左扫描到第一个比给定参数小的值，向右扫描到第一个比给定参数大的值，然后以此为界，构建一个区间)。

利用Next-Key Lock可以阻止其他事务在锁定区间内插入数据，因此在一定程度上解决了幻读问题。

InnoDB通过MVCC实现了在可重复读RR事务隔离级别下不加锁实现快照读的机制，所以所有的行级锁，都不会影响到其他事务中的快照读。

间隙锁存在的目的是为了防止事务执行过程中，另外一个事务对间隙的插入，能够有效避免幻读的发生。
正是因为间隙锁的存在目的，所以多个事务可以同时对同一个间隙加锁，即使它们加的都是排它锁。(事实上，考虑另一种常见情况，事务 1 持有间隙锁 (1， 3]，事务 2 持有间隙锁 (3, 5)，此时将记录 3 删除，那么事务 1 与事务 2 持有的间隙锁都将变成 (1, 5)，如果强制间隙锁的互斥，那么这种情况下就会产生错误)

在读已提交RC与读未提交RU事务隔离级别下，InnoDB会自动禁用间隙锁。

临键锁的加锁场景：
	1. 通过主键或唯一键进行范围查询，会加大于查询范围前开后闭最小范围的临键锁；
	2. 通过非主键或唯一键查询，会锁定对应索引记录及其之前的间隙；
	3. 如果没有建立索引，那么在查询过程中实际上扫描的是全表，所以会锁全表。不过对于select * from tb where *** limit 1这样的语句来说，实际扫描在首次遇到匹配行即结束，所以会锁此行前所有间隙。

MySQL如何避免死锁：
	1. 设置超时。innodb_lock_wait_timeout
	2. 主动死锁检测；通过innodb_deadlock_detect设置为on或off来开启或关闭主动死锁检测机制。
	3. 拆分字段实现单条记录并发度的下降。
	
InnoDB的每个表都会有聚集索引：
	1. 如果定义了主键，主键就是聚集索引；
	2. 如果没有定义主键，则第一个非空unique列是聚集索引；
	3. 否则,InnoDB会创建一个隐藏的row-id作为聚集索引；

分代回收是基于一个事实：对象的生命周期不同，所以针对不同生命周期的对象可以采取不同的回收方式，以便提高回收效率。

GC优化一般步骤可以为：确定目标、优化参数、验收结果。

JVM动态年龄计算：Hotspot遍历所有对象时，按照年龄从小到大对其所占用的大小进行累积，当累积的某个年龄大小超过了survivor区的一半时，取这个年龄和MaxTenuringThreshold中更小的一个值，作为新的晋升年龄阈值。本案例中，调优前：Survivor区=64M, desired survivor=32M，此时Survivor区中age<=2的对象累计大小为41M，42M大于32M，所以晋升年龄阈值被设置为2，下次Minor GC时将年龄超过2的对象被晋升到老年代。

JVM引入动态年龄计算，主要基于如下两点考虑：
	1. 如果固定按照MaxTenuringThreshold设定的阈值作为晋升条件：
		a):MaxTenuringThreshold设置的过大，原本应该晋升的对象一直停留在Survivor区，直到Survivor区溢出，一旦溢出发生，Eden+Survivor中对象将不再依据年龄全部晋升到老年代，这样对象老化的机制就失效了；
		b):MaxTenuringThreshold设置的过小，"过早晋升"即对象不能在新生代充分被回收，大量短期对象被晋升到老年代，老年代空间迅速增长，引起频繁的Major GC。分代回收失去了意义，严重影响GC性能。
	2. 相同应用在不同时间的表现不同：特殊任务的执行或者流量成分的变化，都会导致对象的生命周期分布发生波动，那么固定的阈值设定，因为无法动态适应变化，会造成和上面相同的问题。
总结来看，为了更好的适应不同程序的内存情况，虚拟机并不总是要求对象年龄必须达到MaxTenuringTHreshold再晋升到老年代。

CMS:如果仅扫描老年代中对象，即以老年代中对象为根，判断对象是否存在引用，上图中，对象A因为引用存在新生代中，它在Remark阶段就不会被修正标记为可达，GC时就会被错误回收。
新生代对象持有老年代中对象的引用，这种情况称为"跨代引用"。因为它的存在，Remark阶段必须扫描整个堆来判断对象是否存活。包括图中灰色的不可达对象。
灰色对象已经不可达，但仍然需要扫描的原因：新生代GC和老年代的GC是各自分开独立进行的，只有MinorGC时才会使用根搜索算法，标记新生代对象是否可达，也就是说虽然一些对象已经不可达，但在MinorGC发生前不会被标记为不可达，CMS也无法辨认哪些对象存活，只能全堆扫描(新生代+老年代)。由此可见，堆中对象的数目影响了Remark阶段耗时。

分析GC日志可以得到同样的规律，Remark耗时>500ms时，新生代使用率都在75%以上。这样降低Remark阶段耗时问题转换成如何减少新生代对象数量。

新生代中对象的特点是朝生夕灭，这样如果Remark前执行一次Minor GC，大部分对象就会被回收。CMS就采用了这样的方式，在Remark前增加一个可中断的并发预清理(CMS-concurrent-abortable-preclean)，该阶段主要工作仍然是并发标记对象是否存活，只是这个过程可被中断。此阶段在Eden区使用超过2M时启动，直到Eden区空间使用率达到50%时中断。当然2M和50%都是默认的阈值，可以通过参数修改。如果此阶段执行时等到了Minor GC，那么灰色对象被回收，Remark阶段需要扫描的对象就少了。

除此之外，CMS为了避免这个阶段没有等到Minor GC而陷入无限等待，提供了参数CMSMaxAbortablePrecleanTime，默认为5S，含义是如果可中断的预清理执行超过5S，不管有没有发生Minor GC，都会中止此阶段，进入Remark。

根据GC日志红色标记显式，可中断的并发预清理执行了5.35S,超过了设置的5S被中断，期间没有等到Minor GC，所以Remark时新生代仍然有很多对象。
对于这种情况，CMS提供了CMSScavengeBeforeRemark参数，用来保证Remark前强制进行一次MinorGC。

总结：由于跨代引用的存在，CMS在Remark阶段必须扫描整个堆，同时为了避免扫描时新生代有很多对象，增加了可中断的预清理阶段用来等待Minor GC的发生。只是该阶段有时间限制，如果超时等不到Minor GC，Remark时新生代仍然有很多对象，我们的调优策略是：通过参数强制Remark前进行一次Minor GC，从而降低Remark阶段的时间。
--------------------------------
其实新生代GC同样存在类似的问题，即老年代可能持有新生代对象引用，所以Minor GC时也必须扫描老年代。
JVM是如何避免Minor GC时扫描全堆的?

经过统计信息显示，老年代持有新生代对象引用的情况不足1%，根据这一特性JVM引入了卡表(Card Table)来实现这一目的。

卡表的具体策略是将老年代的空间分成大小为512B的若干张卡(Card)。卡表本身是单字节数组，数组中的每个元素对应着一张卡，当发生老年代引用新生代时，虚拟机将该卡对应的卡表元素设置为适当的值。如图所示，卡表3被标记为脏(卡表还有另外的作用，标识并发标记阶段哪些块被修改过)，之后Minor GC时通过扫描卡表就可以很快识别哪些卡中存在老年代指向新生代的引用。这样JVM通过空间换时间的方式，避免了全堆扫描。
--------------------------------
总结来说，CMS的设计聚焦在获取最短的时延，为此不遗余力地做了很多工作，包括尽量让应用程序和GC线程并发、增加可中断的并发预清理阶段、引入卡表等，虽然这些操作牺牲了一定吞吐量但获得了更短的回收停顿时间。

什么时候可能会触发STW的Full GC?
	1. Perm空间不足；
	2. CMS GC时出现promotion failed和concurrent mode failure(concurrent mode failure发生的原因一般是CMS正在进行，但是由于老年代空间不足，需要尽快回收老年代里面的不再使用的对象，这时停止所有的线程，同时终止CMS，直接进行Serial Old GC)；
	3. 统计得到Young GC晋升到老年代的平均大小大于老年代的剩余空间；
	4. 主动触发Full GC(执行jmap -histo:live <pid>)来避免碎片问题。

对于Redis的底层结构：
	1. 在Cluster模式下，一个Redis实例对应一个RedisDB(db0);
	2. 一个RedisDB对应一个Dict;
	3. 一个Dict对应2个Dictht, 正常情况只用到ht[0];ht[1]在Rehash时使用。

我们知道当HashMap中由于Hash冲突（负载因子）超过某个阈值时，出于链表性能的考虑，会进行Resize的操作。Redis也一样[Redis中通过dictExpand()实现]。

为了高效地匹配出数据库中所有符合给定模式的Key, Redis提供了Scan命令。该命令在每次调用的时候，返回符合规则的部分Key以及一个游标值Cursor(初始值使用0)，使用每次返回Cursor不断迭代，直到Cursor的返回值为0代表遍历结束。


Redis官方定义Scan特点如下：
	1. 整个遍历从开始到结束期间，一直存在于Redis数据集内的且符合匹配模式的所有Key都会被返回；
	2. 如果发生了rehash,同一个元素可能会返回多次，遍历过程中新增或者删除的key可能会被返回，也可能不会。


传统的权限模型有ACL(Access Control List)访问控制列表，RBAC(Role-Based Access Control)基于角色的访问控制等。以上模型比较适用于应用类型产品的权限管控，而数据类型的产品堆信息安全的要求更高，而且各类资源间的关系也更复杂，使用传统的模型难以将内部关系进行清晰的表达，所以需要在RBAC权限模型的基础上，扩展设计新的权限模型。

---------------------------------
Nginx运行CPU亲和力：
worker_processes 4;
worker_cpu_affinity 0001 0010 0100 1000;

Nginx事件处理模型：
events {
	use epoll;
	worker_connections 65535;
	multi_accept on;
}

Nginx的缓存功能有：proxy_cache / fastcgi_cache;
	- proxy_cache的作用是缓存后端服务器的内容，包括静态和动态；
	- fastcgi_cache的作用是缓存fastcgi生成的内容，很多情况是php生成的动态内容。
---------------------------------
2015年反应式(Reactive Stream)规范诞生，定义了如下四个接口：
	1. Subscription接口定义了连接发布者和订阅者的方法
	2. Publisher<T>接口定义了发布者的方法
	3. Subscriber<T>接口定义了订阅者的方法
	4. Processor<T, R>接口定义了处理器

Reactive Stream规范诞生后，RxJava从RxJava2.0开始实现Reactive Stream规范。

Reactive Stream基于流进行处理可以更高效地使用内存，把业务逻辑从模板代码中抽离出来，把代码从并发、同步问题中解脱出来，同时提高了代码的可读性。
Reactive Stream在某些方面是迭代器和观察者模式的结合，同时存在数据的Pull和Push。

JDK9中的Flow类定义了反应式编程的API，实际上就是拷贝了Reactive Stream的四个接口定义，然后放在java.util.concurrent.Flow类中。JDK9提供了SubmissionPublisher和ConsumerSubscriber两个默认实现。

JDK8引入了Stream用于流的操作，JDK9引入的Flow也是数据流的操作。不同点是：Stream更侧重于流的过滤、映射、整合、收集；而Flow更侧重于流的生产和消费。

处理器Processor: Processor位于Publisher和Subscriber之间，用于做数据转换。可以有多个Processer同时使用，组成一个处理链，链中最后一个处理器的处理结果发送给Subscriber。JDK没有提供任何具体的处理器。处理器同时是订阅者和发布者，接口的定义也是继承两者，作为订阅者接收数据，然后进行处理，处理完后作为发送者，再发布出去。

背压back pressure: Subscriber向Publisher请求消息，并通过提供的回调方法被激活调用。如果Publisher的处理能力比Subscriber强得多，需要有一种机制使得Subscriber可以通知Publisher降低生产速度。Publisher实现这种功能的机制被称为背压。提供数据生产者和消费者的消息机制，协调它们之间的产销失衡的情况。JDK9中的Flow API没有提供任何API来发信号或处理背压，需要开发者自行处理。JDK官方建议参考RxJava的背压处理方式。

事件顺序：反应式流中的事件顺序：
	1. 创建发布者和订阅者，分别是Publisher和Subscriber的实例；
	2. 订阅者调用发布者的subscribe进行订阅；
	3. 发布者调用订阅者的onSubscribe传递订阅Subscribtion
	4. 订阅者调用Subscription的request方法请求数据；
	5. 发布者调用订阅者的onNext方法传递数据给订阅者；
	6. 数据传递完成后发布者调用订阅者的onComplete方法通知完成；

Spring Reactor或者Reactive Streams和JDK8 Stream的区别是什么:两者最大的差别是Reactive Streams/Spring Reactor是Push-based, 而JDK8 Stream是Pull-based.
换言之，代码形式并不是Pull与Push的本质。从更深的层面说，Pull模式对应的是同步的、命令式的程序，Push模式对应的是异步的、非阻塞的、反应式的程序。

因此，虽然从代码形式上说JDK8 Stream和Reactive Streams的代码有点像，但从本质上来说，同步、阻塞的JDK8 Stream与异步、非阻塞的Reactive Streams有着很大的差别。

因此Reactive Stream不仅在形式上，以接口定义的形式对反应式编程做出了规范，更在实际层面上定义了TCK，用来保证相关实现确实满足了异步、非阻塞的要求。

自旋锁和适应性自旋锁

无锁：不锁住资源，多个线程中只有一个能修改资源成功，其他线程会重试；
偏向锁：同一个线程执行同步资源时自动获取资源；
轻量级锁：多个线程竞争同步资源时，没有获取资源的线程自旋等待锁释放；
重量级锁：多个线程竞争同步资源时，没有获取资源的线程阻塞等待唤醒；

CAS虽然很高效，但是也存在三大问题：
	1. ABA问题。CAS需要在操作值的时候检查内存值是否发生变化，没有发生变化才会更新内存值。但是如果内存值原来是A，后来变成了B，然后又变成了A，那么CAS进行检查时会发现值没有发生变化，但是实际上是有变化的。ABA问题的解决思路就是在变量前面添加版本号，每次变量更新的时候都把版本号加一，这样变化过程就从“A－B－A”变成了“1A－2B－3A”。JDK从1.5开始提供了AtomicStampedReference类来解决ABA问题，具体操作封装在compareAndSet()中。compareAndSet()首先检查当前引用和当前标志与预期引用和预期标志是否相等，如果都相等，则以原子方式将引用值和标志的值设置为给定的更新值。
	2. 循环时间长开销大。CAS操作如果长时间不成功，会导致一直自旋，给CPU带来非常大的开销。
	3. 只能保证一个共享变量的原子操作。对一个共享变量执行操作时，CAS能够保证原子操作，但是对多个共享变量操作时，CAS是无法保证操作的原子性的。JDK从1.5开始提供AtomicReference类来保证引用对象之间的原子性，可以把多个变量放在一个对象里来进行CAS操作。

自旋锁本身是有缺点的，它不能代替阻塞。自旋等待虽然避免了线程切换的开销，但它要占用处理器时间。如果锁被占用的时间很短，自旋等待的效果就会非常好。反之，如果锁被占用的时间很长，那么自旋的线程只会白浪费处理器资源。所以，自旋等待的时间必须要有一定的限度，如果自旋超过限定次数(默认是10次，可以使用-XX:PreBlockSpin来更改)没有成功获得锁，就应当挂起线程。

自旋锁在JDK1.4.2中引入，使用-XX:+UseSpinning来开启。JDK6变为默认开启，并且引入了自适应的自旋锁(适应性自旋锁)。
自适应意味着自旋的时间(次数)不再固定，而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。如果在同一个锁对象上，自旋等待刚刚获得过锁，并且持有锁的线程正在运行中，那么虚拟机就会任务这次自旋也是很有可能再次成功，进而它将允许自旋等待持续相对更长的时间。如果对于某个锁，自旋很少成功，那在以后尝试获取这个锁时将可能省略掉自旋过程，直接阻塞线程，避免浪费处理器资源。

在自旋锁中，还有三种常见的锁形式：TicketLock, CLHLock和MCSLock。

Hotspot的对象头主要包括两部分数据：Mark Work(标记字段)，Klass Pointer(类型指针)。
Mark Word：默认存储对象的HashCode，分代年龄和锁标志位信息。这些信息都是与对象自身定义无关的数据，所以Mark Word被设计成一个非固定的数据结构以便在极小的空间内存存储尽量多的数据。它会根据对象的状态复用自己的存储空间，也就是说在运行期间Mark Word里存储的数据会随着锁标志位的变化而变化。

Klass Point：对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。

synchronized通过Monitor来实现线程同步，Monitor是依赖于底层的操作系统的Mutex Lock(互斥锁)来实现的线程同步。

四种锁状态对应的Mark Work内容。
锁状态		存储内容											存储内容 
无锁		对象的hashCode,对象分代年龄，是否是偏向锁			01
偏向锁		偏向线程ID，偏向时间戳，对象分代年龄，是否偏向锁	01
轻量级锁 	指向栈中锁记录的指针								00
重量级锁 	指向互斥量(重量级锁)的指针 							10

当一个线程访问同步代码块并获取锁时，会在Mark Word里存储锁偏向的线程ID。在线程进入和退出同步块时不再通过CAS操作来加锁和解锁，而是检测Mark Word里是否存储着指向当前线程的偏向锁。引入偏向锁是为了在无多线程竞争的情况下尽量减少不必要的轻量级锁执行路径，因为轻量级锁的获取及释放依赖多次CAS原子指令，而偏向锁只需要在置换ThreadID的时候依赖一次CAS原子指令即可。

偏向锁只有遇到其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁，线程不会主动释放偏向锁。偏向锁的撤销，需要等待全局安全点（在这个时间点上没有字节码正在执行），它会首先暂停拥有偏向锁的线程，判断锁对象是否处于被锁定状态。撤销偏向锁后恢复到无锁（标志位为“01”）或轻量级锁（标志位为“00”）的状态。

偏向锁在JDK6及之后的版本是默认启用的。可以通过-XX:UseBiasedLocking=false，关闭之后程序默认会进入轻量级锁状态。
-------------------------------------------
轻量级锁是指当锁是偏向锁的时候，被另外的线程所访问，偏向锁就会升级为轻量级锁，其他线程会通过自旋的形式尝试获取锁，不会阻塞，从而提高性能。
当代码进入同步块的时候，如果同步对象状态为无锁状态(锁标志位为：01状态，是否为偏向锁为：0)，虚拟机首先将在当前线程的栈帧中建立一个名为锁记录(Lock Record)的空间，用于存储锁对象目前的MarkWord的拷贝，然后拷贝对象头中的Mark Word复制到锁记录中。
拷贝成功后，虚拟机将使用CAS操作尝试将对象的Mark Word更新为指向Lock Record的指针，并将Lock Record里的Owner指针指向对象的Mark Word。
如果这个更新动作成功了，那么这个线程就拥有了该对象的锁，并且对象Mark Word的锁标志位设置为00，表示此对象处于轻量级锁定状态。
如果轻量级锁的更新操作失败了，虚拟机首先会检查对象的Mark Word是否指向当前线程的栈帧，如果是就说明当前线程已经拥有了这个对象的锁，那就可以直接进入同步块继续执行，否则说明多个线程竞争锁。
若当前只有一个等待线程，则该线程通过自旋进行等待。但是当自旋超过一定的次数，或者一个线程持有锁，一个在自旋，又有第三个来访时，轻量级锁升级为重量级锁。
-------------------------------------------
综上，偏向锁通过对比Mark Word解决加锁问题，避免执行CAS操作。而轻量级锁是通过用CAS操作和自旋来解决加锁问题，避免线程阻塞和唤醒而影响性能。重量级锁是将除了拥有锁的线程以外的线程都阻塞。

公平锁是指多个线程按照申请锁的顺序来获取锁，线程直接进入队列中排队，队列中的第一个线程才能获得锁。公平锁的优点是等待锁的线程不会饿死。缺点是整体吞吐效率相对非公平锁要低，等待队列中除第一个线程以外的所有线程都会阻塞，CPU唤醒阻塞线程的开销比非公平锁大。

非公平锁是多个线程加锁时直接尝试获取锁，获取不到才会到等待队列的队尾等待。但如果此时锁刚好可用，那么这个线程可以无需阻塞直接获取到锁，所以非公平锁有可能出现后申请锁的线程先获取锁的场景。非公平锁的优点是可以减少唤起线程的开销，整体的吞吐效率高，因为线程有几率不阻塞直接获得锁，CPU不必唤醒所有线程。缺点是处于等待队列中的线程可能会饿死，或者等很久才会获得锁。

可重入锁又称递归锁，是指在同一个线程在外层方法获取锁的时候，再进入该线程会自动获取锁(前提锁对象是同一个对象或者class)，不会因为之前已经获取过还没释放而阻塞。Java中ReentrantLock和synchronized都是可重入锁，可重入锁的一个优点是可一定程度避免死锁。
------------------------------------
容器本质上是把系统中同一个业务目标服务的相关进程合成一组，放在一个叫做namespace的空间中，同一个namespace中的进程能够互相通信，同时看不到其他namespace中的进程。每个namespace可以拥有自己独立的主机名、进程ID系统、IPC、网络、文件系统、用户等等资源，在某种程度上，实现了一个简单的虚拟：让一个主机上可以同时运行多个互不感知的系统。
此外，为了限制namespace对物理资源的使用，对进程能使用的CPU、内存等资源需要做一定的限制，这就是Cgroup技术，Cgroup是Control group的简写。比如我们常说的4c4g的容器，实际上是限制这个容器namespace中所用的进程，最多能够使用4核的计算资源和4GB的内存。
简而言之，Linux内核提供namespace完成隔离，Cgroup完成资源限制。namespace+Cgroup构成了容器的底层技术(rootfs是容器文件系统层技术)。
------------------------------------
大家都知道，JVM GC（垃圾对象回收）对Java程序执行性能有一定的影响。默认的JVM使用公式“ParallelGCThreads = (ncpus <= 8) ? ncpus : 3 + ((ncpus * 5) / 8)” 来计算做并行GC的线程数，其中ncpus是JVM发现的系统CPU个数。一旦容器中JVM发现了宿主机的CPU个数（通常比容器实际CPU限制多很多），这就会导致JVM启动过多的GC线程，直接的结果就导致GC性能下降。Java服务的感受就是延时增加，TP监控曲线突刺增加，吞吐量下降。

Netty底层基于JDK的NIO，为什么不直接基于JDK的NIO或者其他NIO框架：
	1. 使用JDK自带的NIO需要了解太多概念，编程复杂；
	2. Netty底层IO模型随意切换，而这一切只需要做微小的改动；
	3. Netty自带的拆包解包、异常检测等机制可以从NIO的繁重细节中脱离出来，只需关注业务逻辑；
	4. Netty解决了JDK的很多包括空轮询在内的Bug;
	5. Netty底层对线程,Selector做了很多细小的优化，精心设计的Reactor线程做到非常高效的并发处理；
	6. 自带各自协议栈，通用协议栈可以不用再次开发；
	7. Netty社区活跃，可随时邮件列表或提issue;
	8. Netty已经被各大RPC框架(Dubbo)、消息中间件(RocketMQ)、大数据通信(Hadoop)框架广泛的线上验证，健壮性强大。

在开源社区中，满足CP的产品：etcd,Zookeeper,Consule等。

Etcd底层Key的存储为BTree结构，查找时间复杂度为O(logN)，百万级甚至千万级Key的查找耗时区别不大。

MySQL里经常说的WAL技术: Write-Ahead Logging，关键点是先写日志，再写磁盘。write pos是当前记录的位置，一边写一边后移，写到第3号文件末尾后回到0号文件开头。checkpoint是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。
有了redo log，InnoDB就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe.

redo log是InnoDB引擎特有的日志，而Server层也有自己的日志，称为binlog(归档日志)。

为什么会有redo log和binlog两份日志：最开始MySQL并没有InnoDB引擎。MySQL自带的引擎是MyISM，但是MyISM没有crash-safe的能力，binlog日志只能用于归档。而InnoDB是另一个公司以插件形式引入MySQL的，既然只依靠binlog是没有crash-safe能力的，所以InnoDB使用另外一套日志系统: redolog来实现crash-safe能力。

redo log和binlog的三点区别：
	1. redo log是InnoDB引擎特有的；binlog是MySQL的Server层实现的，所有引擎都可以使用；
	2. redo log是物理日志，记录的是“在某个数据页上做了什么修改”；binlog是逻辑日志，记录的是这个语句的原始逻辑；
	3. redo log是循环写的，空间固定会用完；binlog是可以追加写入的。追加写是指binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。
	
由于redo log和binlog是两个独立的逻辑，如果不用两阶段提交，要么就是先写完redo log再写binlog，或者采用反过来的顺序。我们看看这两种方式会有什么问题。

仍然用前面的update语句来做例子。假设当前ID=2的行，字段c的值是0，再假设执行update语句过程中在写完第一个日志后，第二个日志还没有写完期间发生了crash，会出现什么情况呢？

先写redo log后写binlog。假设在redo log写完，binlog还没有写完的时候，MySQL进程异常重启。由于我们前面说过的，redo log写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行c的值是1。
但是由于binlog没写完就crash了，这时候binlog里面就没有记录这个语句。因此，之后备份日志的时候，存起来的binlog里面就没有这条语句。
然后你会发现，如果需要用这个binlog来恢复临时库的话，由于这个语句的binlog丢失，这个临时库就会少了这一次更新，恢复出来的这一行c的值就是0，与原库的值不同。

先写binlog后写redo log。如果在binlog写完之后crash，由于redo log还没写，崩溃恢复以后这个事务无效，所以这一行c的值是0。但是binlog里面已经记录了“把c从0改成1”这个日志。所以，在之后用binlog来恢复的时候就多了一个事务出来，恢复出来的这一行c的值就是1，与原库的值不同。

可以看到，如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的状态不一致。简单说，redo log和binlog都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致。

MySQL里面最重要的两个日志：物理日志redo和逻辑日志binlog。
redo log用于保证crash-safe能力。innodb_flush_log_at_trx_commit这个参数设置成1的时候，表示每次事务的redo log都直接持久化到磁盘。这个参数我建议你设置成1，这样可以保证MySQL异常重启之后数据不丢失。

sync_binlog这个参数设置成1的时候，表示每次事务的binlog都持久化到磁盘。这个参数我也建议你设置成1，这样可以保证MySQL异常重启之后binlog不丢失。

消除依赖、弱化依赖、控制依赖
事务中不包含外部调用

接收数据包的大致需要以下几个步骤：
	1. 网卡收到数据包；
	2. 将数据包从网卡硬件缓存转移到服务器内存中；
	3. 通知内核处理；
	4. 经过TPC/IP协议逐层处理；
	5. 应用程序通过read()从socket buffer读取数据。

火焰图是根据调用栈的样本集生产的可视化性能分析图。需要重点关注平顶，那里就是程序的CPU热点。调用树是另一种可视化分析的手段，与火焰图一样，也是根据同一份样本集而生成，按需选择即可。

JVMI(JVM Tool Interface)是JVM提供的一套标准的C/C++编程接口，是实现Debugger, Profiler, Monitor, Thread Analyser等工具的统一基础，在主流JVM中都有实现。
JNIEXPORT jint JNICALL Agent_OnLoad(JavaVM *vm, char *options, void *reserved);
使用C/C++实现该函数，并将代码编译为动态链接库(Linux为.so)，通过-agentpath参数将库的完整路径传递给Java进程，JVM就会在启动阶段的合适时机执行该函数。在函数内部，可以通过JavaVM指针参数拿到JNI和JVMTI的函数指针表，就拥有了与JVM进行各种复杂交互的能力。

在很多场景下，我们没有必要必须使用C/C++来开发JVMTI Agent，因为成本高且不易维护。JVM自身基于JVMTI封装了一套Java的Instrument API接口，允许使用Java语言开发Java Agent（只是一个jar包），大大降低了Agent的开发成本。社区开源的产品如Greys, Arthas, JVM-Sandbox, JVM-Profiler等都是纯Java编写的，也是以Java Agent形式来运行。
在Java Agent中，需要在jar包的MANIFEST.MF中的Premain-Class指定为一个入口类。
public static void premain(String[] args, Instrumentation ins) {
	//implement
}
在该方法内部，参数Instrumentation接口提供了Retransform Classes的能力，利用该接口就可以对宿主进程的Class进行修改，实现方法耗时统计、故障注入、Trace等功能。Instrumentation接口提供的能力较为单一，仅与Class字节码操作相关，但已经处于宿主进程环境内，就可以利用JMX直接获取宿主进程的内存、线程、锁等信息。无论是Instrument API还是JMX，它们内部仍是统一基于JVMTI来实现的。

只有在分场景讨论下才有意义。Sampling由于低开销的特性，更适合用在CPU密集型的应用中，以及不可接受大量性能开销的线上服务中。而Instrumentation则更适合用在IO密集型的应用中、对性能开销不敏感以及确实需要精确统计的场景中。社区中的Profiler更多的是基于Sampling来实现。

都是替换已经存在的class文件，redefineClasses是自己提供字节码文件替换掉已存在的class文件，retransformClasses是在已存在的字节码文件上修改后再替换之。

BTrace是基于Java语言的一个安全的、可提供动态追踪服务的工具。BTrace基于ASM、Java Attach API、Instrument开发，为用户提供了很多注解。依靠这些注解，我们可以编写BTrace脚本（简单的Java代码）达到我们想要的效果，而不必深陷于ASM对字节码的操作中不可自拔。



基于Instrument和Attach API前辈们创造出了诸如JProfiler、Jvisualvm、BTrace这样的工具。以ASM为基础发展出了cglib、动态代理，继而是应用广泛的Spring AOP。

Java是静态语言，运行时不允许改变数据结构。然而，Java 5引入Instrument，Java 6引入Attach API之后，事情开始变得不一样了。

------------------------------------------------------
			ReentrantLock					synchronized 
灵活性 		支持响应中断、超时、尝试取锁	不灵活
锁类型 		公平锁&非公平锁					非公平锁
条件队列	可关联多个条件队列				关联一个条件队列
锁实现机制	依赖AQS							监视器模式
可重入性 	可重入 							可重入
释放形式	必须显式unlock()				自动释放监视器 
------------------------------------------------------
AQS核心思想是，如果被请求的共享资源空闲，那么就将当前请求资源的线程设置为有效的工作线程，将共享资源设置为锁定状态；如果共享资源被占用，就需要一定的阻塞等待唤醒机制来保证锁分配。这个机制主要用的是CLH队列的变体实现的，将暂时获取不到锁的线程加入到队列中。
CLH：Craig、Landin and Hagersten队列，是单向链表，AQS中的队列是CLH变体的虚拟双向队列（FIFO），AQS是通过将每条请求共享资源的线程封装成一个节点来实现锁的分配。
AQS使用一个volatile的int类型的成员变量来表示同步状态，通过内置的FIFO队列来完成资源获取的排队工作，通过CAS完成对state值的修改。
AQS中的Node的waitStatus的几个枚举值：
	CANCELLED:为1，表示线程获取锁的请求已经取消了
	SIGNAL：为-1，表示线程已经准备好了，就等资源释放了
	CONDITION: 为-2，表示节点在等待队列中，节点线程等待唤醒
	PROPAGATE: 为-3，当前线程处于SHARED情况下，该字段才会使用
	0: 当一个Node被初始化的时候的默认值。

对cancelled节点状态的产生和变化有了了解，但是为什么所有的变化都是对Next指针进行操作，而没有对prev指针进行操作呢？什么情况下会会prev指针进行操作？
	1. 执行cancelAcquire的时候，当前节点的前置节点可能已经从队列中出去了(已经执行了try代码块中的shouldParkAfterFailedAcquire方法了)，如果此时修改prev指针，有可能会导致prev指向另一个移出队列的Node，因此这时变化prev指针不安全；
	2. shouldParkAfterFailedAcquire方法中，会执行下面的代码，其实就是在处理prev指针。shouldParkAfterFailedAcquire是获取锁失败的情况下才会执行，进入该方法后，说明共享资源已被获取，当前节点之前的节点都不会发生变化。因此这个时候变更prev指针比较安全。
	do {
		node.prev = pred = pred.prev;
	} while (pred.waitStatus > 0);


用DDD可以很好地解决领域模型到设计模型的同步、演化，最后再将反映了领域的设计模型转为实际的代码。
模型是我们解决实际问题所抽象出来的概念模型，领域模型则表达与业务相关的室是；设计模型则描述了所要构建的系统。

简单的业务系统采用贫血模型和过程化设计是没有问题的，但在业务逻辑复杂，业务逻辑、状态散落到大量方法中，原本的代码意图会渐渐不明确，我们将这种情况称为由贫血症引起的失忆症。
更好的是采用领域模型的开发方式，将数据和行为封装在一起，并与现实世界中的业务对象相映射。各类具备明确的职责划分，将领域逻辑分散到领域对象中。

解决复杂和大规模软件的武器可以被粗略地归为三类：抽象、分治和知识。

DDD的核心诉求是将业务架构映射到系统架构上，在响应业务变化调整业务架构时，也随之变化系统架构。而微服务追求业务层面的复用，设计出来的系统架构和业务一致；在技术架构上则系统模块之间充分解耦，可以自由地选择合适的技术架构，去中心化地治理技术和数据。

JVMTI是一套Native接口，在Java SE5之前，要实现一个Agent只能通过编写Native代码来实现。从Java SE5开始，可以使用Java的Instrumentation接口(java.lang.instrument)来编写Agent。无论是通过Native的方式还是通过Java Instrumentation接口的方式来编写Agent，它们的工作都是借助JVMTI来完成。

首先通过指定的进程ID找到目标JVM，然后通过Attach挂载到目标JVM上，执行加载Agent操作。VirtualMachine的Attach方法就是用来将Agent挂载到目标JVM上去的，而Detach则是将Agent从目标JVM卸载。

static AttachOperationFunctionInfo funcs[] = {
	{"agentProperties", get_agent_properties},
	{"datadump", data_dump},
	{"dumpheap", dump_heap},
	{"load", load_agent},
	{"properties", get_system_properties},
	{"threaddump", thread_dump},
	{"inspectheap", heap_inspection},
	{"setflag", set_flag},
	{"printflag", print_flag},
	{"jcmd", jcmd},
	{NULL, NULL}
};

findSocketFile方法用来查询目标JVM上是否已经启动了Attach Listener，它通过检查"tmp/"目录下是否在java_pid{pid}来进行实现。如果已经存在了，则说明Attach机制已经准备就绪，可以接受客户端的命令了，这个时候客户端就可以通过connect连接到目标JVM进行命令的发送，比如可以发送"load"命令来加载Agent。如果java_pid{pid}文件还不存在，则需要通过sendQuitTo方法向目标JVM发送一个"SIGBREAK"信号，让它初始化Attach Listener线程并准备接受客户端连接。可以看出，发送了信号之后客户端会循环等待java_pid{pid}这个文件，之后再通过connect连接到目标JVM上。

LruCache在美团DSP系统的应用场景
在美团DSP系统中广泛应用键值存储数据库，例如使用Redis存储广告信息，服务可以通过广告ID获取广告信息。每次请求都从远端的键值存储数据库中获取广告信息，请求耗时非常长。随着业务发展，QPS呈现巨大的增长趋势，在这种高并发的应用场景下，将广告信息从远端键值存储数据库中迁移到本地以减少查询耗时是常见解决方案。另外服务本身的内存占用要稳定在一个安全的区间内。面对持续增长的广告信息，引入LruCache + 键值存储数据库的机制来达到提高系统性能，维持内存占用安全、稳定的目标。

缓存命中率/缓存占用空间/缓存的逐出策略/缓存的过期策略

回滚日志不能一直保留，什么时候删除呢？答案是，在不需要的时候才删除。也就是说，系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除。
什么时候才不需要了呢？就是当系统里没有比这个回滚日志更早的read-view的时候。
基于上面的说明，不建议使用长事务，原因如下：
	1. 长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。
在MySQL5.5及以前的版本，回滚日志是跟数据字典一起放在ibdata文件里的，即使长事务最终提交，回滚段被清理，文件也不会变小。有些数据只有20G，而回滚段有200GB的库。最终只好为了清理回滚段，重建整个库。
	2. 除了回滚段，长事务还占用锁资源，也可能拖垮整个库。

可以在information_schema库的innodb_trx这个表中查询长事务，如下查询持续时间大于60秒的事务：
select * from information_schema.innodb_trx where time_to_sec(timediff(now(), trx_started)) > 60;
-------------------------------------------------------
电梯调度算法:
	1. 先来先服务算法: First Come First Service
	2. 最短寻找楼层时间优先:Shorted Seek Time First 
	3. 扫描算法: Scan
	4. Look算法：look算法是扫描算法的一种改进。对look算法而言，电梯同样在最底层和最顶层之间运行。但当look算法发现电梯所移动的方向上不再有请求时立即改变运行方向，而扫描算法则需要移动到最底层或者最顶层时才改变运行方向；
	5. SATF算法：最短访问时间优先
-------------------------------------------------------
B+树能够很好地配合磁盘的读写特性，减少单次查询的磁盘访问次数。

如何避免长事务对业务的影响：
	首先，从应用开发端来看：
		1. 确认是否使用了set autocommit=0。这个确认可以在测试环境开展，把MySQL的general_log开起来，通过general_log的日志来确认。一般框架设置了这个值，也就会提供参数来控制，目标就是改为1；
		2. 确认是否有不必要的只读事务。有些框架会习隔把任何语句都begin/commit框起来。某些业务不需要，比如多个select语句放在事务中。这种只读事务可以去掉。
		3. 业务连接数据库时候，根据业务评估，通过 set max_execution_time命令，来控制每个语句执行的最长时间，避免单个语句意外执行太长时间。
	其次，从数据库端来看：
		1. 监控information_schema.innodb_tr表，设置长事务阈值，超过就报警或者kill；
		2. percona的pt-kill工具；
		3. 在业务功能测试阶段输出所有的general_log，分析日志行为提前发现问题；
		4. 如果是MySQL5.6或者更新版本，把innodb_undo_tablespaces设置成2（或更大的值）。如果真的出现大事务导致回滚段过大，这样设置后清理起来更方便。

在MySQL5.6引入的索引下推优化(index condition pushdown)，可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤到不满足条件的记录，减少回表次数。

为什么要重建索引：索引可能因为删除，或者页分裂等原因，导致数据页有空洞，重建索引的过程会创建一个新的索引，把数据按顺序插入，这样页面的利用率最高，也就是索引更紧凑、更省空间。

MySQL里的锁：全局锁、表锁和行锁。

全局锁：就是对整个数据库实例加锁。MySQL提供了一个加全局锁的方法:flush tables with read lock(FTWRL);当需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的语句会被阻塞：数据的更新语句(数据的增删改)、数据定义语句(DML)和更新类事务的提交语句。
全局锁的典型使用场景是，做全局逻辑备份。

MySQL官方自带的逻辑备份工具是mysqldump。当mysqldump使用参数--single-transaction的时候，导数据之前会启动一个事务，来确保拿到一致性事务。而由于MVCC的支持，这个过程中数据可以正常更新。
你一定在疑惑，有了这个功能，为什么还需要FTWRL呢？一致性读是好，但前提是引擎要支持这个隔离级别。比如，对于MyISAM这种不支持事务的引擎，如果备份过程中有更新，总是只能取到最新的数据，那么就破坏了备份的一致性。这时，我们就需要使用FTWRL命令了。

MySQL表级别的锁有两种：表锁和元数据锁(meta data lock, MDL)

因此，在MySQL 5.5版本中引入了MDL，当对一个表做增删改查操作的时候，加MDL读锁；当要对表做结构变更操作的时候，加MDL写锁。

在InnoDB事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。

当出现死锁以后，有两种策略：

一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数innodb_lock_wait_timeout来设置。
另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数innodb_deadlock_detect设置为on，表示开启这个逻辑。
在InnoDB中，innodb_lock_wait_timeout的默认值是50s，意味着如果采用第一个策略，当出现死锁以后，第一个被锁住的线程要过50s才会超时退出，然后其他线程才有可能继续执行。对于在线服务来说，这个等待时间往往是无法接受的。

但是，我们又不可能直接把这个时间设置成一个很小的值，比如1s。这样当出现死锁的时候，确实很快就可以解开，但如果不是死锁，而是简单的锁等待呢？所以，超时时间设置太短的话，会出现很多误伤。

所以，正常情况下我们还是要采用第二种策略，即：主动死锁检测，而且innodb_deadlock_detect的默认值本身就是on。主动死锁检测在发生死锁的时候，是能够快速发现并进行处理的，但是它也是有额外负担的。
-----------------------------
InnoDB里面每个事务有一个唯一的事务ID，叫做transaction id。它是在事务开始的时候向InnoDB的事务系统申请的，是按照申请顺序严格递增的。
而每行数据也都有多个版本。每次事务更新数据的时候，都会生成一个新的数据版本，并且把transaction id赋值给这个数据版本的事务ID，记为row trx_id。同时，旧的数据版本要保留，并且在新的数据版本中，能够有信息直接拿到它。
也就是说，数据表中的一行记录，其实可能有多个版本(row)，每个版本有自己的row trx_id。

在实现上，InnoDB为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在"活跃"的所有事务ID。活跃指的是，启动了但还没提交。
数组里面事务ID的最小值为低水位，当前系统里面已经创建过的事务ID的最大值加1记为高水位。
这个视图数组和高水位，就组成了当前事务的一致性视图(consistent read-view)。
而数据版本的可见性规则，就是基于数据的row trx_id和这个一致性视图的对比结果得到的。

用到一条规则：更新数据都是先读后写的，而这个读，只能读当前的值，称为"当前读"(current read)。
提到一个概念，叫做当前读。其实，除了update语句外，select语句如果加锁，也是当前读。
所以，如果把事务A的查询语句select * from t where id=1修改一下，加上lock in share mode或for update，也都能读到最新的数据。

事务的可重复读的能力是怎么实现的？
	可重复读的核心就是一致性读（consistent read）；而事务更新数据的时候，只能用当前读。如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。

而读提交和逻辑和可重复读的逻辑类似，它们最主要的区别是：
	1. 在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图；
	2. 在读已提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。

-----------------------------