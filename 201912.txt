Sentinel: 流量控制+流量整形+熔断降级+系统自适应保护

API网关出现的原因是微服务架构的出现，不同的微服务会有不同的网络地址，而外部客户端需要调用多个服务的接口才能完成一个业务需求，如果让客户端直接与各个微服务通信，会有以下问题：
	1. 客户端会多次请求不同的微服务，增加了客户端的复杂性；
	2. 存在跨域请求，在一定场景下处理相对复杂；
	3. 认证复杂，每个服务都需要独立认证；
	4. 难以重构，随着项目迭代，可能需要重新划分微服务。例如，多个服务合并成一个或者将一个拆分成多个。如果客户端直接与各个微服务通信，那么重构将很难实施；
	5. 某些微服务可能使用了防火墙/浏览器不友好的协议，直接访问有困难。

Spring Cloud Gateway features:
	1. Built on Spring Framework 5, Project Reactor and Spring Boot 2.0;
	2. Able to match routes on any request attribute.
	3. Predicates and filters are specific to routes.
	4. Hystrix Circuit Breaker integration.
	5. Spring Cloud DiscoveryClient integration.
	6. Easy to write Predicates and Filters.
	7. Request Rate Limiting;
	8. Path Rewriting.

Spring Cloud Gateway的核心：
	1. Route: The basic building block of the gateway. It is defined by an ID, a destination URI, a collection of predicates, and a collection of filters. A route is matched if the aggregate predicate is true.
	2. Predicate: This is a Java 8 Function Predicate. The input type is a Spring Framework ServerWebExchange. This lets you match on anything from the HTTP request, such as headers or parameters.
	3. Filter: There are instance of Spring Framework GatewayFilter that have been constructed with a specific factory. Here, you can modify requests and responses before or after sending the downstream request. 

Clients make requests to Spring Cloud Gateway. If the Gateway Handler Mapping determines that a request matches a route, it is sent to the Gateway Web Handler. This handler runs the request through a filter chain that is specific to the request. The reason the filters are divided by the dotted line is that filters can run logic both before and after the proxy request is sent. All "pre" filter logic is executed. Then the proxy request is made. After the proxy request is made, the "post" filter logic is run. 

内置的Route Predicate Factories:
	1. The After Route Predicate Factory;
	2. The Before Route Predicate Factory;
	3. The Between Route Predicate Factory;
	4. The Cookie Route Predicate Factory;
	5. The Header Route Predicate Factory;
	6. The Host Route Predicate Factory;
	7. The Method Route Predicate Factory;
	8. The Path Route Predicate Factory;
	9. The Query Route Predicate Factory;
	10. The RemoteAddr Route Predicate Factory;
	11. The Weight Route Predicate Factory;
--------------------------------------------------
The Redis RateLimiter:
A steady rate is accomplished by setting the same value in replenishRate and burstCapacity. Temporary bursts can be allowed by setting burstCapacity higher than replenishRate. In this case, the rate limiter needs to be allowed some time between bursts(according to replenishRate), as two consecutive bursts will result in dropped requests(HTTP 429 - Too Many Request). The following listing configures a redis-rate-limiter:
Rate limits bellow 1 request/s are accomplished by setting replenishRate to the wanted number of requests, requestedTokens to the timespan in seconds and burstCapacity to the product of replenishRate and requestedTokens, e.g. setting replenishRate=1, requestedTokens=60 and burstCapacity=60 will result in a limit of 1 request/min.
--------------------------------------------------
内置的GatewayFilter Factories:
	1. The AddRequestHeader GatewayFilter Factory;
	2. The AddRequestParameter GatewayFilter Factory;
	3. The AddResponseHeader GatewayFilter Factory;
	4. The DedupeResponseHeader GatewayFilter Factory;
	5. The Hystrix GatewayFilter Factory;
	6. Spring Cloud CircuitBreaker GatewayFilter Factory;
	7. The FallbackHeaders GatewayFilter Factory;
	8. The MapRequestHeader GatewayFilter;
	9. The PrefixPath GatewayFilter Factory;
	10. The PreserveHostHeader GatewayFilter Factory;
	11. The RequestRateLimiter GatewayFilter Factory;
	12. The RedirectTo GatewayFilter Factory;
	13. The RemoveRequestHeader GatewayFilter Factory;
	14. The RemoveResponseHeader GatewayFilter Factory;
	15. The RemoveRequestParameter GatewayFilter Factory;
	16. The RewritePath GatewayFilter Factory;
	17. The RewriteLocationResponseHeader GatewayFilter Factory;
	18. The RewriteResponseHeader GatewayFilter Factory;
	19. The SaveSession GatewayFilter Factory;
	20. The SecureHeaders GatewayFilter Factory;
	21. The SetPath GatewayFilter Factory;
	22. The SetRequestHeader GatewayFilter Factory;
	23. The SetResponseHeader GatewayFilter Factory;
	24. The SetStatus GatewayFilter Factory;
	25. The StripPrefix GatewayFilter Factory;
	26. The Retry GatewayFilter Factory;
	27. The RequestSize GatewayFilter Factory;
	28. Modify a Request Body GatewayFilter Factory;
	29. Modify a Response Body GatewayFilter Factory;
	30. Default Filters;

Zuul is a JVM-based router and server-side load balancer from Netflix. 
Netflix uses Zuul for the following:
	1. Authentication;
	2. Insights;
	3. Stress Testing;
	4. Canary Testing;
	5. Dynamic Routing;
	6. Service Migration;
	7. Load Shedding;
	8. Security;
	9. Static Response handling;
	10. Active traffic management;
Zuul's rule engine lets rules and filters be written in essentially any JVM language, with built-in support for Java and Groovy. 

注意：在网络分裂出现期间，客户端C1可以向主节点B发送写命令的最大时间是有限制的，这一时间限制称为节点超时时间(node timeout)，是Redis集群的一个重要配置选项。
开启集群的Redis节点的最小配置如下：
port 7000
cluster-enabled yes
cluster-config-file nodes.conf 
cluster-node-timeout 5000
appendonly yes 

用例图是指由参与者(Actor)、用例(Use Case)、边界以及它们之间的关系构成的用于描述系统功能的视图。用例图(Use Case)是外部用户所能观察到的系统功能的模型图。用例图是系统的蓝图。用例图呈现了一些参与者，一些用例，以及它们之间的关系，主要用于对系统、子系统或类的功能行为进行建模。

Zuul: If you would like to provide a default fallback for all routes, you can create a bean of type #FallbackProvider and have the #getRoute method return * or null, as shown in the following example:
class MyFallbackProvider implements FallbackProvider {
	@Override
	public String getRoute() {
		return "*";
	}
	
	@Override 
	public ClientHttpResponse fallbackResponse(String route, Throwable throwable) {
		return new ClientHttpResponse() {
			@Override
			public HttpStatus getStatusCode() throws IOException {
				return HttpStatus.OK;
			}
			@Override
            public int getRawStatusCode() throws IOException {
                return 200;
            }
            @Override
            public String getStatusText() throws IOException {
                return "OK";
            }
            @Override
            public void close() {

            }
            @Override
            public InputStream getBody() throws IOException {
                return new ByteArrayInputStream("fallback".getBytes());
            }
            @Override
            public HttpHeaders getHeaders() {
                HttpHeaders headers = new HttpHeaders();
                headers.setContentType(MediaType.APPLICATION_JSON);
                return headers;
            }
		};
	}
}

If Zuul is fronting a web application, you may need to re-write the Location header when the web application redirects through a HTTP status code of 3XX. Otherwise, the browser redirects to the web application's URL instead of the Zuul URL. You can configure a LocationRewriteFilter Zuul filter to re-write the Location header to the Zuul's URL. It also adds back the stripped golbal and route-specific prefixes. 

The Zuul Servlet: Zuul is implemented as a Servlet. For the general cases, Zuul is embedded into the Spring Dispatch mechanism. This lets Spring MVC be in control of the routing. In this case, Zuul buffers requests. If there is a need to go through Zuul without buffering requests(for example, for large file uploads), the Servlet is also installed outside of the Spring Dispatcher. By default, the servlet has an address of /zuul. This patch can be changed with the zuul.servlet-path property. 

To pass information between filters, Zuul uses a RequestContext. Its data is held in a ThreadLocal specific to each request. Information about where to route requests, errors, and the actual HttpServletRequest and HttpServletResponse are stored there. Thre RequestContext extends ConcurrentHashMap, so anything can be stored in the context. FilterConstants contains the keys used by the filters installed by Spring Cloud Netflix. 

Spring Cloud Netfilx installs a number of filters, depending on which annotation was used to enable Zuul. @EnableZuulProxy is a superset of @EnableZuulServer. In other words, @EnableZuulProxy contains all the filters installed by @EnableZuulServer. The additional filters int the "proxy" enable routing functionality. If you want a "blank" Zuul, you should use @EnableZuulServer. 

@EnableZuulServer Filters: 
@EnableZuulServer creates a SimpleRouteLocator that loads route definitions from Spring Boot configuration files. 
	1. Pre filters:
		1.1 ServletDetectionFilter
		1.2 FormBodyWrapperFilter
		1.3 DebugFilter
	2. Route filters:
		2.1 SendForwardFilter
	3. Post filters:
		3.1 SendResponseFilter
	4. Error filters:
		4.1 SendErrorFilter

@EnableZuulProxy Filters:
Creates a DiscoveryClientRouteLocator that loads route definitions from a DiscoveryClient(such as Eureka) as well as from properties. A route is created from each serviceId from the DiscoveryClient. As new services are added, the routes are refreshed.
In addition to the filters described earlier, the following filters are installed(as normal Spring Beans):
Pre filters:
	PreDecorationFilter
Route filters:
	RibbonRoutingFilter
	SimpleHostRoutingFilter

Zuul Eager ApplicationContext Loading:Zuul internally uses Ribbon for calling the remote URLs. By default, Ribbon clients are lazily loaded by Spring Cloud on first call. This behavior can be changed for Zuul by using configuration: zuul.ribbon.earger-load.enabled=true, which results eager loading of the child Ribbon related Application contexts at startup time. 

Spring Cloud Gateway Global Filters:
	1. Combined Global Filter and GatewayFilter Ordering;
	2. Forward Routing Filter;
	3. The LoadBalancerClient Filter;
	4. The ReactiveLoadBalancerClientFilter;
	5. The Netty Routing Filter;
	6. The Netty Write Response Filter;
	7. The RouteToRequestUrl Filter;
	8. The Websocket Routing Filter;
	9. The Gateway Metrics Filter;
	10. Marking An Exchange As Routed.
------------------------------------------------------------
Spring Cloud Gateway: 
In order to write a Route Predicate you will need to implement RoutePredicateFactory. There is an abstract class called AbstractRoutePredicateFactory which you can extend.
To write a GatewayFilter, you must implement GatewayFilterFactory. You can extend an abstract class called AbstractGatewayFilterFactory.
To write a custom global filter, you must implement GlobalFilter interface. This applies the filter to all requests. 
------------------------------------------------------------
public interface BeanFactory {
	String FACTORY_BEAN_PREFIX = "&";
	Object getBean(String name) throws BeansException;
	<T> T getBean(String name, Class<T> requiredType) throws BeansException;
	Object getBean(String name, Object... args) throws BeansException;
	<T> T getBean(Class<T> requiredType) throws BeansException;
	<T> T getBean(Class<T> requiredType, Object... args) throws BeansException;
	<T> ObjectProvider<T> getBeanProvider(Class<T> requiredType);
	<T> ObjectProvider<T> getBeanProvider(ResolvableType requiredType);
	boolean containsBean(String name);
	boolean isSingleton(String name) throws NoSuchBeanDefinitionException;
	boolean isPrototype(String name) throws NoSuchBeanDefinitionException;
	boolean isTypeMatch(String name, ResolableType typeToMatch) throws NoSuchBeanDefinitionException;
	boolean isTypeMatch(String name, Class<?> typeToMatch) throws NoSuchBeanDefinitionException;
	@Nullable
	Class<?> getType(String name) throws NoSuchBeanDefinitionException;
	String[] getAliases(String name);
}

public interface FactoryBean<T> {
	@Nullable
	T getObject() throws Exception;
	@Nullable 
	Class<?> getObjectType();
	default boolean isSingleton() {return true;}
}

BeanFactory和FactoryBean的区别:
BeanFactory是接口，提供了IOC最基本的实现，给具体的IOC容器提供了规范，也就是Spring IOC所遵守的最底层和最基本的编程规范。它的职责包括：实例化、定位、配置应用程序中的对象及简历这些对象间的依赖。BeanFactory是Factory,也就是容器工厂或对象工厂，在Spring中，所有的对象都是由BeanFactory管理的。在Spring代码中，BeanFactory只是接口，并不是IOC容器的具体实现，但是Spring给出了很多实现，比如DefaultListableBeanFactory, XmlBeanFactory, ApplicationContext等，都是附加了某种功能的实现。
但是对于FactoryBean而言，这个Bean不是简单的bean，它是能生产或装饰对象的bean。一般情况下，Spring通过反射机制利用<bean>的class属性指定实现类实例化Bean，在某些情况下，实例化Bean过程比较复杂，配置方式的灵活性受到限制，FactoryBean提供更加灵活的实现方式，FactoryBean在Bean的实现方式上，加了一个工厂模式，一个装饰者模式。FactoryBean接口对于Spring框架非常重要，Spring自身提供了70多种FactoryBean的实现。它们隐藏了实例化复杂Bean的细节，给上层应用带来了便利。

事务注解的注意事项：
	1. 一个功能是否需要事务，必须纳入设计、编码考虑。不能仅完成基本功能；
	2. 如果加了事务，必须做好测试(尽量触发异常、测试回滚)，确保事务生效。
	3. @Transactional的注意事项
		3.1 尽量不要在接口上用@Transactional，而要在具体类上使用@Transactional注解，否则注解可能无效；
		3.2 将@Transactional放在类级别上，会使得所有方法都有事务。故@Transactional应该放在方法级别上，不需要使用事务的方法，就不要放置事务，比如某些查询方法，否则对性能有影响；
		3.3 使用了@Transactional的方法，对同一个类里面的方法调用，@Transactional无效。
		3.4 使用@Transactional方法，只能是public, @Transactional注解的方法都是被外部其他类调用才有效，故只能是public。
	4. 正确的设置propagation, isolation, timeout, readOnly, rollbackFor, rollbackForClassName, noRollbackFor;

解决hash冲突的方法:
	1. 开放定址法(线性探测再散列/平方探测再散列/随机探测再散列)；
	2. 链地址法；
	3. 再hash法；
	4. 建立公共溢出区;

refresh()方法是ConfigurableApplicationContext接口提供的方法，refresh()方法的具体实现是由AbstractApplicationContext这个抽象类实现的，AbstractApplicationContext这个抽象类继承了DefaultResourceLoader，并且实现了接口ConfigurableAppplicationContext。

用过哪些RPC框架，讲讲他们优缺点：
	1. UP的Magpai;
	2. RMI;
	3. Dubbo;
	4. Spring Cloud Feign;
	5. Motan;
	6. gRPC；
	7. Hessian;
	8. WebService:两种常用实现:CXF和Axis2
	9. Apache Thrift
	
信号量模式从始至终都只有请求线程自身，是同步调用模式，不支持超时调用，不支持直接熔断，由于没有线程的切换，开销非常小。
线程池模式可以支持异步调用，支持超时调用，支持直接熔断，存在线程切换，开销大。
------------------------------------------------------------------------
hystrix:
  command: #用于控制HystrixCommand的行为
    default:
      execution:
        isolation:
          strategy: THREAD #控制HystrixCommand的隔离策略，THREAD->线程池隔离策略(默认)，SEMAPHORE->信号量隔离策略
          thread:
            timeoutInMilliseconds: 1000 #配置HystrixCommand执行的超时时间，执行超过该时间会进行服务降级处理
            interruptOnTimeout: true #配置HystrixCommand执行超时的时候是否要中断
            interruptOnCancel: true #配置HystrixCommand执行被取消的时候是否要中断
          timeout:
            enabled: true #配置HystrixCommand的执行是否启用超时时间
          semaphore:
            maxConcurrentRequests: 10 #当使用信号量隔离策略时，用来控制并发量的大小，超过该并发量的请求会被拒绝
      fallback:
        enabled: true #用于控制是否启用服务降级
      circuitBreaker: #用于控制HystrixCircuitBreaker的行为
        enabled: true #用于控制断路器是否跟踪健康状况以及熔断请求
        requestVolumeThreshold: 20 #超过该请求数的请求会被拒绝
        forceOpen: false #强制打开断路器，拒绝所有请求
        forceClosed: false #强制关闭断路器，接收所有请求
      requestCache:
        enabled: true #用于控制是否开启请求缓存
  collapser: #用于控制HystrixCollapser的执行行为
    default:
      maxRequestsInBatch: 100 #控制一次合并请求合并的最大请求数
      timerDelayinMilliseconds: 10 #控制多少毫秒内的请求会被合并成一个
      requestCache:
        enabled: true #控制合并请求是否开启缓存
  threadpool: #用于控制HystrixCommand执行所在线程池的行为
    default:
      coreSize: 10 #线程池的核心线程数
      maximumSize: 10 #线程池的最大线程数，超过该线程数的请求会被拒绝
      maxQueueSize: -1 #用于设置线程池的最大队列大小，-1采用SynchronousQueue，其他正数采用LinkedBlockingQueue
      queueSizeRejectionThreshold: 5 #用于设置线程池队列的拒绝阀值，由于LinkedBlockingQueue不能动态改版大小，使用时需要用该参数来控制线程数
 
实例配置
 
实例配置只需要将全局配置中的default换成与之对应的key即可。
hystrix:
  command:
    HystrixComandKey: #将default换成HystrixComrnandKey
      execution:
        isolation:
          strategy: THREAD
  collapser:
    HystrixCollapserKey: #将default换成HystrixCollapserKey
      maxRequestsInBatch: 100
  threadpool:
    HystrixThreadPoolKey: #将default换成HystrixThreadPoolKey
      coreSize: 10 
HystrixComandKey对应@HystrixCommand中的commandKey属性；
HystrixCollapserKey对应@HystrixCollapser注解中的collapserKey属性
------------------------------------------------------------------------
Hystrix配置的默认值是在HystrixCommandProperties, HystrixCollapserProperties, HystixThreadPoolProperties类。

To provide a default configuration for all of your circuit breakers create a Customizer bean that is passed a HystrixCircuitBreakerFactory or ReactiveHystrixCircuitBreakerFactory. The configureDefault method can be used to provide a default configuration. 
@Bean 
public Customizer<HystrixCircuitBreakerFactory> defaultConifg() {
	return factory -> factory.configureDefault(id -> HystrixCommand.Setter
			.withGroupKey(HystrixCommandGroupKey.Factory.asKey(id))
			.andCommandPropertiesDefaults(HystrixCommandProperties.Setter()
			.withExecutionTimeoutInMilliseconds(4000)));
}

HystrixObservableCommand
A service failure in the low level of services can cause cascading failure all the way up to the user. When calls to a particular service exceed circuitBreaker.requestVolumeThreshold(default: 20 requests) and the failure percentage is greater than circuitBreaker.errorThresholdPercentage(default:>50%) in a rolling window defined by metrics.rollingStats.timeInMilliseconds(default: 10 seconds), the circuit opens and the call is not made. In cases of error and an open circuit, a fallback can be provided by the developer. 

Looking at an individual instance’s Hystrix data is not very useful in terms of the overall health of the system. Turbine is an application that aggregates all of the relevant /hystrix.stream endpoints into a combined /turbine.stream for use in the Hystrix Dashboard. 

熔断器Hystrix最主要的作用:
	1. 资源隔离；
	2. 熔断；
	3. 降级；
	4. 限流；
	5. 缓存；
	6. 请求合并；

MySQL的各种高可用方案，大多是基于下面几种基础来部署的：
	1. 基于主从复制；
	2. 基于Galera协议；
	3. 基于NDB引擎；
	4. 基于中间件/proxy;
	5. 基于共享存储；
	6. 基于主机高可用；

Galera是Codership提供的多主数据同步复制机制，可以实现多个节点间的数据同步复制以及读写，并且可保障数据库的服务高可用及数据一致性。
基于Galera的高可用方案主要有MariaDB Galera Cluster和Percona XtraDB Cluster(简称PXC)，目前PXC用的比较多一些。
在PXC中，一次数据写入在各个节点间的验证/回滚流程。
PXC的优点：
	1. 服务高可用；
	2. 数据同步复制(并发复制)，几乎无延迟；
	3. 多个可同时读写节点，可实现写扩展，不过最好事先进行分库分表，让各个节点分别写不同的表或库，避免让Galera解决数据冲突；
	4. 新节点可以自动部署，部署操作简单；
	5. 数据严格一致性，尤其适合电商类应用；
	6. 完全兼容MySQL;
PXC的局限性：
	1. 只支持InnoDB引擎；
	2. 所有表都要有主键；
	3. 不支持Lock Table等显式锁操作；
	4. 锁冲突、死锁问题相对更多；
	5. 不支持XA；
	6. 集群吞吐量/性能取决于短板；
	7. 新加入节点采用SST时，代价高；
	8. 存在写扩大问题；
	9. 如果并发事务量很大的话，建议采用InfinBand网络，降低网络延迟；
事实上，采用PXC的主要目的是解决数据的一致性问题，高可用是顺带事先的。因为PXC存在写扩大以及短板效应，并发效率会有较大损失，类似于semi sync replication机制。

MySQL多主复制管理器(MMM)
MySQL异步复制有如下问题：
	1. slave服务器的数据集总是落后于master；
	2. MySQL复制很慢-它从二进制日志回访事务；
对于Galera，事务是在它们被提交之前被所有节点确认。如果一个事务在一个节点失败了，那个节点将立刻从集群移除。换句话说，Galera主从复制是同步点。永远不会丢失事务-没有延迟(而且Galera的基于行的复制大约要快5倍).

MHA Manager可以单独部署在一台独立的机器上管理多个master-slave集群，也可以部署在一台slave节点上。MHA Node运行在每台MySQL服务器上，MHA Manager会定时探测集群中的master节点，当master出现故障时，它可以自动将最新数据的slave提升为新的master，然后将所有其他的slave重新指向新的master。整个故障转移过程对应用程序完全透明。

/* A cancellable asynchronous computation. This class provides a base implementation of #Future, with methods to start and cancel a computation, query to see if the computation is complete, and retrieve the result of the computation. The result can only be retrieved when the computation has completed; the #get methods will block if the computation has not yet completed. Once the computation has completed, the computation cannot be restarted or cancelled (unless the computation is invoked using #runAndReset).
A #FutureTask can be used to wrap a #Callable or #Runnable object. Because #FutureTask implements #Runnable, a #FutureTask can be submitted to an #Executor for execution. 
In addition to serving as a standalone class, this class provides #protected functionality that may be useful when creating customized task classes. 
*/
public class FutureTask<V> implements RunableFuture<V> {
	/*
     * Revision notes: This differs from previous versions of this
     * class that relied on AbstractQueuedSynchronizer, mainly to
     * avoid surprising users about retaining interrupt status during
     * cancellation races. Sync control in the current design relies
     * on a "state" field updated via CAS to track completion, along
     * with a simple Treiber stack to hold waiting threads.
     *
     * Style note: As usual, we bypass overhead of using
     * AtomicXFieldUpdaters and instead directly use Unsafe intrinsics.
     */

    /**
     * The run state of this task, initially NEW.  The run state
     * transitions to a terminal state only in methods set,
     * setException, and cancel.  During completion, state may take on
     * transient values of COMPLETING (while outcome is being set) or
     * INTERRUPTING (only while interrupting the runner to satisfy a
     * cancel(true)). Transitions from these intermediate to final
     * states use cheaper ordered/lazy writes because values are unique
     * and cannot be further modified.
     *
     * Possible state transitions:
     * NEW -> COMPLETING -> NORMAL
     * NEW -> COMPLETING -> EXCEPTIONAL
     * NEW -> CANCELLED
     * NEW -> INTERRUPTING -> INTERRUPTED
     */
    private volatile int state;
    private static final int NEW          = 0;
    private static final int COMPLETING   = 1;
    private static final int NORMAL       = 2;
    private static final int EXCEPTIONAL  = 3;
    private static final int CANCELLED    = 4;
    private static final int INTERRUPTING = 5;
    private static final int INTERRUPTED  = 6;

    /** The underlying callable; nulled out after running */
    private Callable<V> callable;
    /** The result to return or exception to throw from get() */
    private Object outcome; // non-volatile, protected by state reads/writes
    /** The thread running the callable; CASed during run() */
    private volatile Thread runner;
    /** Treiber stack of waiting threads */
    private volatile WaitNode waiters;
	...
	
	public V get() throws InterruptedException, ExecutionException {
		int s = state;
		if (s <= COMPLETING)
			s = awaitDone(false, 0L);
		return report(s);
	}
	
	public V get(long timeout, TimeUnit unit)
        throws InterruptedException, ExecutionException, TimeoutException {
        if (unit == null)
            throw new NullPointerException();
        int s = state;
        if (s <= COMPLETING &&
            (s = awaitDone(true, unit.toNanos(timeout))) <= COMPLETING)
            throw new TimeoutException();
        return report(s);
    }
	
    /**
     * Protected method invoked when this task transitions to state
     * {@code isDone} (whether normally or via cancellation). The
     * default implementation does nothing.  Subclasses may override
     * this method to invoke completion callbacks or perform
     * bookkeeping. Note that you can query status inside the
     * implementation of this method to determine whether this task
     * has been cancelled.
     */	
	protected void done() {}
	
	/**
     * Sets the result of this future to the given value unless
     * this future has already been set or has been cancelled.
     *
     * <p>This method is invoked internally by the {@link #run} method
     * upon successful completion of the computation.
     *
     * @param v the value
     */
    protected void set(V v) {
        if (UNSAFE.compareAndSwapInt(this, stateOffset, NEW, COMPLETING)) {
            outcome = v;
            UNSAFE.putOrderedInt(this, stateOffset, NORMAL); // final state
            finishCompletion();
        }
    }
}

/* 
...
Keep-alive times
If the pool currently has more than corePoolSize threads, excess threads will be terminated if they have been idle for more than keepAliveTime(#getKeepAliveTime(TimeUnit)). This provides a means of reducing resource consumption when the pool is not being actively used. If the pool becomes more active later, new threads will be constructed. This parameter can also be changed dynamically using method #setKeepAliveTime(long, TimeUnit). Using a value of Long.MAX_VALUE effectively disable idle threads from ever terminating prior to shut down. By default, the keep-alive policy applies only when there are more than corePoolSize threads. But method #allowCoreThreadTimeout(boolean) can be used to apply this time-out policy to core threads as well, so long as the keepAliveTime value is non-zero.

* There are three general strategies for queuing:
 * <ol>
 *
 * <li> <em> Direct handoffs.</em> A good default choice for a work
 * queue is a {@link SynchronousQueue} that hands off tasks to threads
 * without otherwise holding them. Here, an attempt to queue a task
 * will fail if no threads are immediately available to run it, so a
 * new thread will be constructed. This policy avoids lockups when
 * handling sets of requests that might have internal dependencies.
 * Direct handoffs generally require unbounded maximumPoolSizes to
 * avoid rejection of new submitted tasks. This in turn admits the
 * possibility of unbounded thread growth when commands continue to
 * arrive on average faster than they can be processed.  </li>
 *
 * <li><em> Unbounded queues.</em> Using an unbounded queue (for
 * example a {@link LinkedBlockingQueue} without a predefined
 * capacity) will cause new tasks to wait in the queue when all
 * corePoolSize threads are busy. Thus, no more than corePoolSize
 * threads will ever be created. (And the value of the maximumPoolSize
 * therefore doesn't have any effect.)  This may be appropriate when
 * each task is completely independent of others, so tasks cannot
 * affect each others execution; for example, in a web page server.
 * While this style of queuing can be useful in smoothing out
 * transient bursts of requests, it admits the possibility of
 * unbounded work queue growth when commands continue to arrive on
 * average faster than they can be processed.  </li>
 *
 * <li><em>Bounded queues.</em> A bounded queue (for example, an
 * {@link ArrayBlockingQueue}) helps prevent resource exhaustion when
 * used with finite maximumPoolSizes, but can be more difficult to
 * tune and control.  Queue sizes and maximum pool sizes may be traded
 * off for each other: Using large queues and small pools minimizes
 * CPU usage, OS resources, and context-switching overhead, but can
 * lead to artificially low throughput.  If tasks frequently block (for
 * example if they are I/O bound), a system may be able to schedule
 * time for more threads than you otherwise allow. Use of small queues
 * generally requires larger pool sizes, which keeps CPUs busier but
 * may encounter unacceptable scheduling overhead, which also
 * decreases throughput.  </li>
 
 Rejected tasks 
 New tasks submitted in method #execute(Runnable) will be rejected when the Executor has been shut down, and also when the Executor uses finite bounds for both maximum threads and work queue capacity, and is saturated. In either case, the #execute method invokes the RejectedExecutionHandler#rejectedExecution(Runnable, ThreadPoolExecutor) method of its RejectedExecutionHandler. 
...
*/
public class ThreadPoolExecutor extends AbstractExecutorService {
	    /**
     * The main pool control state, ctl, is an atomic integer packing
     * two conceptual fields
     *   workerCount, indicating the effective number of threads
     *   runState,    indicating whether running, shutting down etc
     *
     * In order to pack them into one int, we limit workerCount to
     * (2^29)-1 (about 500 million) threads rather than (2^31)-1 (2
     * billion) otherwise representable. If this is ever an issue in
     * the future, the variable can be changed to be an AtomicLong,
     * and the shift/mask constants below adjusted. But until the need
     * arises, this code is a bit faster and simpler using an int.
     *
     * The workerCount is the number of workers that have been
     * permitted to start and not permitted to stop.  The value may be
     * transiently different from the actual number of live threads,
     * for example when a ThreadFactory fails to create a thread when
     * asked, and when exiting threads are still performing
     * bookkeeping before terminating. The user-visible pool size is
     * reported as the current size of the workers set.
     *
     * The runState provides the main lifecycle control, taking on values:
     *
     *   RUNNING:  Accept new tasks and process queued tasks
     *   SHUTDOWN: Don't accept new tasks, but process queued tasks
     *   STOP:     Don't accept new tasks, don't process queued tasks,
     *             and interrupt in-progress tasks
     *   TIDYING:  All tasks have terminated, workerCount is zero,
     *             the thread transitioning to state TIDYING
     *             will run the terminated() hook method
     *   TERMINATED: terminated() has completed
     *
     * The numerical order among these values matters, to allow
     * ordered comparisons. The runState monotonically increases over
     * time, but need not hit each state. The transitions are:
     *
     * RUNNING -> SHUTDOWN
     *    On invocation of shutdown(), perhaps implicitly in finalize()
     * (RUNNING or SHUTDOWN) -> STOP
     *    On invocation of shutdownNow()
     * SHUTDOWN -> TIDYING
     *    When both queue and pool are empty
     * STOP -> TIDYING
     *    When pool is empty
     * TIDYING -> TERMINATED
     *    When the terminated() hook method has completed
     *
     * Threads waiting in awaitTermination() will return when the
     * state reaches TERMINATED.
     *
     * Detecting the transition from SHUTDOWN to TIDYING is less
     * straightforward than you'd like because the queue may become
     * empty after non-empty and vice versa during SHUTDOWN state, but
     * we can only terminate if, after seeing that it is empty, we see
     * that workerCount is 0 (which sometimes entails a recheck -- see
     * below).
     */
    private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));
    private static final int COUNT_BITS = Integer.SIZE - 3;
    private static final int CAPACITY   = (1 << COUNT_BITS) - 1;

    // runState is stored in the high-order bits
    private static final int RUNNING    = -1 << COUNT_BITS;
    private static final int SHUTDOWN   =  0 << COUNT_BITS;
    private static final int STOP       =  1 << COUNT_BITS;
    private static final int TIDYING    =  2 << COUNT_BITS;
    private static final int TERMINATED =  3 << COUNT_BITS;

    // Packing and unpacking ctl
    private static int runStateOf(int c)     { return c & ~CAPACITY; }
    private static int workerCountOf(int c)  { return c & CAPACITY; }
    private static int ctlOf(int rs, int wc) { return rs | wc; }
	...
	/* Decrements the workerCount field of ctl. This is called only on abrupt termination of 
		a thread (see processWorkerExit). Other decrements are performed within getTask.
	*/
	private void decrementWorkerCount() {
		do {} while (! compareAndDecrementWorkerCount(ctl.get())));
	}
	...
	private final BlockingQueue<Runnable> workQueue;
	...
	/* Lock held on access to workers set and related bookkeeping.
		While we could use a concurrent set of some sort, it turns out to be 
		gengerally preferable to use a lock. Among the reasons is that this serializes
		interruptIdleWorkers, which avoids unnecessary interrupt storms, especially during shutdown.
		Otherwise exiting threads would concurrently interrupt those that have not yet interrupted.
		It also simplifies some of the associated statistics bookkeeping of largestPoolSize etc.
		We also hold mainLock on shutdown and shutdownNow, for the sake of ensuring workers set is 
		stable while separately checking permission to interrupt and actually interrupting.  
	*/
	private final ReetrantLock mainLock = new ReentrantLock();
	/**
     * Set containing all worker threads in pool. Accessed only when
     * holding mainLock.
     */
    private final HashSet<Worker> workers = new HashSet<Worker>();

    /**
     * Wait condition to support awaitTermination
     */
    private final Condition termination = mainLock.newCondition();

    /**
     * Tracks largest attained pool size. Accessed only under
     * mainLock.
     */
    private int largestPoolSize;

    /**
     * Counter for completed tasks. Updated only on termination of
     * worker threads. Accessed only under mainLock.
     */
    private long completedTaskCount;
	/* All user control parameters are declared as volatile so that 
		ongoing actions are based on freshest values, but without need for locking, 
		since no internal invariants depend on them changing synchronously with 
		respect to other actions.
	*/
	/**
     * Timeout in nanoseconds for idle threads waiting for work.
     * Threads use this timeout when there are more than corePoolSize
     * present or if allowCoreThreadTimeOut. Otherwise they wait
     * forever for new work.
     */
    private volatile long keepAliveTime;

    /**
     * If false (default), core threads stay alive even when idle.
     * If true, core threads use keepAliveTime to time out waiting
     * for work.
     */
    private volatile boolean allowCoreThreadTimeOut;

    /**
     * Core pool size is the minimum number of workers to keep alive
     * (and not allow to time out etc) unless allowCoreThreadTimeOut
     * is set, in which case the minimum is zero.
     */
    private volatile int corePoolSize;

    /**
     * Maximum pool size. Note that the actual maximum is internally
     * bounded by CAPACITY.
     */
    private volatile int maximumPoolSize;

    /**
     * The default rejected execution handler
     */
    private static final RejectedExecutionHandler defaultHandler =
        new AbortPolicy();
	...
	
	/* Class Worker mainly maintains interrupt control state for threads running tasks, 
		along with other minor bookkeeping. This class opportunistically extends AbstractQueuedSynchronizer 
		to simplify acquiring and releasing a lock surrounding each task execution. This protects against 
		interrupts that are intended to wake up a worker thread waiting for a task from instead 
		interrupting a task being run. We implement a simple non-reentrant mutual exclusion lock 
		rather than use ReentrantLock because we do not want worker tasks to be able to reacquire the lock 
		when they invoke pool control methods like setCorePoolSize. Additionally, to suppress interrupts 
		until the thread actually starts running tasks, we initialize lock state to a negative value, 
		and clear it upon start(in #runWorker).
	*/
	private final class Worker extends AbstractQueuedSynchronizer implements Runnable {
		...
	}
	
	final void runWorker(Worker w) {
		Thread wt = Thread.currentThread();
		Runnable task = w.firstTask;
		w.firstTask = null;
		w.unlock();   // allow interrupts 
		boolean completedAbruptly = true;
		try {
			while (task != null || (task = getTask()) != null) {
				w.lock();
				// If pool is stopping, ensure thread is interrupted;
				// if not, ensure thread is not interrupted. This requires a recheck 
				// in second case to deal with shutdownNow race while clearing interrupt 
				if ((runStateAtLeast(ctl.get(), STOP) || 
					(Thread.interrupted() && runStateAtLeast(ctl.get(), STOP))) 
						&& !wt.isInterrupted())
					wt.interrupt();
				try {
					beforeExecute(wt, task);
					Throwable thrown = null;
					try {
						task.run();
					} catch (RuntimeException x) {
						thrown = x; throw x;
					} catch (Error x) {
						thrown = x; throw x;
					} catch (Throwable x) {
						thrown = x; throw new Error(x);
					} finally {
						afterExecute(task, thrown);
					}
				} finally {
					task = null;
					w.completedTasks ++;
					w.unlock();
				}
			}
			completedAbruptly = false;
		} finally {
			processWorkerExit(w, completedAbruptly);
		}
	}
	...
	/**
     * Sets the core number of threads.  This overrides any value set
     * in the constructor.  If the new value is smaller than the
     * current value, excess existing threads will be terminated when
     * they next become idle.  If larger, new threads will, if needed,
     * be started to execute any queued tasks.
     *
     * @param corePoolSize the new core size
     * @throws IllegalArgumentException if {@code corePoolSize < 0}
     * @see #getCorePoolSize
     */
    public void setCorePoolSize(int corePoolSize) {
        if (corePoolSize < 0)
            throw new IllegalArgumentException();
        int delta = corePoolSize - this.corePoolSize;
        this.corePoolSize = corePoolSize;
        if (workerCountOf(ctl.get()) > corePoolSize)
            interruptIdleWorkers();
        else if (delta > 0) {
            // We don't really know how many new threads are "needed".
            // As a heuristic, prestart enough new workers (up to new
            // core size) to handle the current number of tasks in
            // queue, but stop if queue becomes empty while doing so.
            int k = Math.min(delta, workQueue.size());
            while (k-- > 0 && addWorker(null, true)) {
                if (workQueue.isEmpty())
                    break;
            }
        }
    }
	...
	/**
     * Starts a core thread, causing it to idly wait for work. This
     * overrides the default policy of starting core threads only when
     * new tasks are executed. This method will return {@code false}
     * if all core threads have already been started.
     *
     * @return {@code true} if a thread was started
     */
    public boolean prestartCoreThread() {
        return workerCountOf(ctl.get()) < corePoolSize &&
            addWorker(null, true);
    }
	/**
     * Same as prestartCoreThread except arranges that at least one
     * thread is started even if corePoolSize is 0.
     */
    void ensurePrestart() {
        int wc = workerCountOf(ctl.get());
        if (wc < corePoolSize)
            addWorker(null, true);
        else if (wc == 0)
            addWorker(null, false);
    }
	    /**
     * Starts all core threads, causing them to idly wait for work. This
     * overrides the default policy of starting core threads only when
     * new tasks are executed.
     *
     * @return the number of threads started
     */
    public int prestartAllCoreThreads() {
        int n = 0;
        while (addWorker(null, true))
            ++n;
        return n;
    }
	
	
}

	

